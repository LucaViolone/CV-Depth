{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucaViolone/CV-Depth/blob/main/clip_zero_shot_anatomy1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EGOIHA08dn-"
      },
      "source": [
        "### Project README — CLIP zero-shot anatomy tagging + attention maps\n",
        "\n",
        "- Note* make sure to download dataset and change the IMG_dir path to the folder in colab: https://drive.google.com/drive/folders/1QMRI0YUqyZVYA9Aig_ISmS6hRIYQI34G?usp=sharing\n",
        "**Idea**: Zero-shot classification and localization of anatomical concepts with CLIP.\n",
        "- **Method**: Use CLIP to score prompts like \"arachnoid mater,\" \"dura,\" \"cranial nerve.\" Visualize Grad-CAM (RN50) or attention rollout (ViT) to localize. Optionally calibrate with 0–5 labeled images via temperature scaling or prompt ensembling.\n",
        "- **Demo**: Provide an image set; do prompt engineering and ensembling; display top-k labels and heatmaps.\n",
        "- **Why low-data**: Text supervision replaces labels.\n",
        "- **How to run**: \"Run All\" the notebook. Add images to `images/` (or use provided Wikimedia examples). Optional: supply a tiny labeled set for calibration.\n",
        "- **Outputs**: Per-image top-k label scores and Grad-CAM overlays; optional accuracy if labels exist; discuss domain shift and prompt failures.\n",
        "- **File**: `clip_zero_shot_anatomy.ipynb`.\n",
        "- **References**: CLIP (Radford et al., 2021) — https://arxiv.org/abs/2103.00020; Transformer interpretability — https://arxiv.org/pdf/2002.05709.\n",
        "### - LUCA VIOLONE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsH30Q6U8doC"
      },
      "source": [
        "# CLIP Zero-Shot Anatomy Tagging + Saliency Maps\n",
        "\n",
        "This notebook demonstrates zero-shot classification and localization for anatomical concepts using CLIP. We score a set of anatomy prompts against images and visualize where the model \"looks\" using Grad-CAM (for the RN50 CLIP variant) to provide spatial saliency maps.\n",
        "\n",
        "- Paper: Contrastive Language–Image Pretraining (CLIP) — https://arxiv.org/abs/2103.00020\n",
        "- Goal: Show how text supervision enables low-data recognition and produce interpretable heatmaps.\n",
        "- Contents: setup, prompt engineering, zero-shot scoring, Grad-CAM saliency, examples, and commentary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Hvdr8wi8doE",
        "outputId": "128d7f73-7b68-47b8-e1aa-36d133b1fa22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q open-clip-torch==2.24.0 ftfy==6.2.3 regex==2024.9.11 torch torchvision timm==1.0.9 matplotlib pillow opencv-python tqdm\n",
        "# Check environment\n",
        "import torch, platform\n",
        "print(\"Python\", platform.python_version())\n",
        "print(\"Torch\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peFbPLuX8doI"
      },
      "outputs": [],
      "source": [
        "import os, sys, math, random, json, glob, urllib.request\n",
        "import io\n",
        "import numpy as np\n",
        "import torch # Add torch import back\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import open_clip\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Reproducibility\n",
        "\n",
        "def set_seed(seed: int = 3) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(3)\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "if device == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAxX5gGf8doJ"
      },
      "outputs": [],
      "source": [
        "# Load CLIP (OpenCLIP) model and preprocess\n",
        "# We will use RN50 for Grad-CAM support; ViT models are supported for attention rollout, but Grad-CAM is simpler on conv backbones.\n",
        "model_name = \"RN50\"\n",
        "pretrained = \"openai\"\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained, device=device)\n",
        "tokenizer = open_clip.get_tokenizer(model_name)\n",
        "model.eval();\n",
        "\n",
        "print(f\"Loaded {model_name} ({pretrained})\")\n",
        "\n",
        "# Labels and domain phrases\n",
        "base_labels = [\n",
        "    \"arachnoid mater\",\n",
        "    \"dura mater\",\n",
        "    \"cranial nerve\",\n",
        "    \"optic nerve\",\n",
        "    \"cerebellum\",\n",
        "    \"brainstem\",\n",
        "    \"sulcus\",\n",
        "    \"gyrus\",\n",
        "]\n",
        "\n",
        "label_to_phrases = {\n",
        "    \"arachnoid mater\": [\"arachnoid mater\", \"arachnoid membrane\", \"subarachnoid space\"],\n",
        "    \"dura mater\": [\"dura mater\", \"dural membrane\", \"dura\"],\n",
        "    \"cranial nerve\": [\"cranial nerve\", \"cranial nerve root\"],\n",
        "    \"optic nerve\": [\"optic nerve\", \"cranial nerve II\", \"optic nerve head\", \"optic disc\"],\n",
        "    \"cerebellum\": [\"cerebellum\", \"cerebellar hemisphere\", \"vermis\"],\n",
        "    \"brainstem\": [\"brainstem\", \"midbrain\", \"pons\", \"medulla oblongata\"],\n",
        "    \"sulcus\": [\"cerebral sulcus\", \"sulcus\"],\n",
        "    \"gyrus\": [\"cerebral gyrus\", \"gyrus\"],\n",
        "}\n",
        "\n",
        "prompt_templates = [\n",
        "    \"an intraoperative photograph of {}\",\n",
        "    \"an operating microscope photo of {}\",\n",
        "    \"an endoscopic surgical view of {}\",\n",
        "    \"a neurosurgical anatomy photo of {}\",\n",
        "    \"a medical photograph of {}\",\n",
        "    \"a close-up photo of {}\",\n",
        "    \"an MRI of {}\"\n",
        "]\n",
        "\n",
        "# Expand prompts per label using phrases/synonyms\n",
        "label_to_prompts = {\n",
        "    label: [tmpl.format(phrase) for phrase in label_to_phrases[label] for tmpl in prompt_templates]\n",
        "    for label in base_labels\n",
        "}\n",
        "\n",
        "all_texts = sum(label_to_prompts.values(), [])  # flattened list\n",
        "print(\"Num labels:\", len(base_labels), \"Num prompts:\", len(all_texts))\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_tokens = tokenizer(all_texts)\n",
        "    text_features = model.encode_text(text_tokens.to(device))\n",
        "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPaKDoxz8doL"
      },
      "outputs": [],
      "source": [
        "IMG_DIR = '/content/dataset' #change to path of dataset folder download: https://drive.google.com/drive/folders/1QMRI0YUqyZVYA9Aig_ISmS6hRIYQI34G?usp=sharing\n",
        "\n",
        "# Define the image file extensions you want to find\n",
        "image_extensions = ('*.jpg', '*.png', '*.jpeg', '*.gif')\n",
        "\n",
        "# Collect all image paths from the folder\n",
        "image_paths = []\n",
        "for ext in image_extensions:\n",
        "    image_paths.extend(glob.glob(os.path.join(IMG_DIR, ext)))\n",
        "\n",
        "image_paths.sort() # Ensure the list is sorted\n",
        "print(\"Found images:\", len(image_paths))\n",
        "print(image_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h2bZd5A8doM"
      },
      "outputs": [],
      "source": [
        "# Zero-shot scoring utilities\n",
        "# Goal: encode an image and compare it to each text prompt; aggregate prompt scores by label.\n",
        "\n",
        "def preprocess_pil(img: Image.Image) -> torch.Tensor:\n",
        "    # Applies the CLIP preprocessing (resize/crop/normalize) and adds a batch dimension.\n",
        "    return preprocess(img).unsqueeze(0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def score_image_against_prompts(img: Image.Image, text_features: torch.Tensor, all_texts: list[str]) -> dict:\n",
        "    # 1) Encode image to the CLIP embedding space\n",
        "    image_input = preprocess_pil(img).to(device)\n",
        "    image_features = model.encode_image(image_input)\n",
        "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "    # 2) Compare image vector to every text vector (cosine similarity via dot product after norm)\n",
        "    logits = (100.0 * image_features @ text_features.T)[0].float()\n",
        "    # 3) Softmax over prompts gives a probability mass across all prompts\n",
        "    probs = logits.softmax(dim=-1).cpu().numpy()\n",
        "    return {prompt: float(p) for prompt, p in zip(all_texts, probs)}\n",
        "\n",
        "# Aggregate prompt probabilities per label via mean (or max) across that label's prompts.\n",
        "def aggregate_by_label(scores: dict, label_to_prompts: dict, reduce: str = \"mean\") -> dict:\n",
        "    label_scores = {}\n",
        "    for label, prompts in label_to_prompts.items():\n",
        "        vals = [scores.get(p, 0.0) for p in prompts]\n",
        "        if reduce == \"mean\":\n",
        "            label_scores[label] = float(np.mean(vals))\n",
        "        else:\n",
        "            label_scores[label] = float(np.max(vals))\n",
        "    return label_scores\n",
        "\n",
        "# Display top-k labels\n",
        "\n",
        "def show_topk(label_scores: dict, k: int = 5):\n",
        "    top = sorted(label_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "    for label, p in top:\n",
        "        print(f\"{label:18s}  {p:.3f}\")\n",
        "\n",
        "# Demo: score each image and show the top-5 labels.\n",
        "for img_path in image_paths:\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    print(\"\\nImage:\", os.path.basename(img_path))\n",
        "    scores = score_image_against_prompts(img, text_features, all_texts)\n",
        "    label_scores = aggregate_by_label(scores, label_to_prompts, reduce=\"mean\")\n",
        "    show_topk(label_scores, k=5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAzGp_xv8doO"
      },
      "source": [
        "### Educational: Zero-shot scoring and prompt ensembling\n",
        "- Concept: CLIP embeds images and texts into the same space. We compare an image to many prompts and interpret higher cosine similarities as better matches.\n",
        "- Prompt ensembling: averaging (or max) over multiple phrasings per label reduces sensitivity to wording.\n",
        "- What you’ll see: a bar chart of label scores for a chosen image, then we proceed to saliency to understand where the evidence comes from.\n",
        "- To create a saliency map with CLIP, a query and an image are given to the model. The method then analyzes how different regions of the image contribute to the overall similarity score between the image and the text query.\n",
        "- The resulting map highlights which parts of the image were most influential in the model's judgment that the image matches the text description. For example, if the query is \"a cat,\" the saliency map would likely highlight the pixels corresponding to the cat in the image. This helps to explain what the model is \"looking at\" when it makes a prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bz6f6ZT8doO"
      },
      "outputs": [],
      "source": [
        "# Visualization: bar chart of label scores for a selected image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if len(image_paths) > 0:\n",
        "    sel_idx = 0  # change index to visualize a different image\n",
        "    img_path = image_paths[sel_idx]\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    scores = score_image_against_prompts(img, text_features, all_texts)\n",
        "    label_scores = aggregate_by_label(scores, label_to_prompts, reduce=\"mean\")\n",
        "\n",
        "    labels = list(label_scores.keys())\n",
        "    vals = np.array([label_scores[k] for k in labels], dtype=float)\n",
        "    order = np.argsort(-vals)\n",
        "    labels_sorted = [labels[i] for i in order]\n",
        "    vals_sorted = vals[order]\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.barh(labels_sorted[::-1], vals_sorted[::-1], color='steelblue')\n",
        "    plt.xlabel('Probability (softmax over prompts)')\n",
        "    plt.title(f'Label scores: {os.path.basename(img_path)}')\n",
        "    plt.tight_layout(); plt.show()\n",
        "else:\n",
        "    print(\"No images found. Add images and rerun.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pzvYCu08doP"
      },
      "outputs": [],
      "source": [
        "# Grad-CAM for CLIP RN50 image encoder\n",
        "# We'll hook the last conv features and the linear projection to get class agnostic saliency wrt the max scoring prompt.\n",
        "# Idea: find which spatial regions (conv feature maps) most influence the score for a chosen prompt.\n",
        "\n",
        "import types\n",
        "\n",
        "def get_rn50_visual(model):\n",
        "    # open_clip RN50 visual backbone is in model.visual; final conv layer is layer4[-1].conv3 for ResNet-50\n",
        "    return model.visual\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        visual = get_rn50_visual(model)\n",
        "        target_layer = visual.layer4[-1].conv3  # last conv layer in ResNet-50\n",
        "\n",
        "        # Save conv activations on forward and gradients on backward.\n",
        "        def fwd_hook(module, inp, out):\n",
        "            self.activations = out.detach()\n",
        "        def bwd_hook(module, grad_in, grad_out):\n",
        "            self.gradients = grad_out[0].detach()\n",
        "\n",
        "        self.fh = target_layer.register_forward_hook(fwd_hook)\n",
        "        self.bh = target_layer.register_full_backward_hook(bwd_hook)\n",
        "\n",
        "    def remove(self):\n",
        "        self.fh.remove(); self.bh.remove()\n",
        "\n",
        "    def __call__(self, img_tensor: torch.Tensor, text_feature: torch.Tensor):\n",
        "        # Forward: get image embedding; normalize for cosine-like comparison.\n",
        "        img_tensor = img_tensor.requires_grad_(True)\n",
        "        image_features = self.model.encode_image(img_tensor)\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        # Scalar score: similarity between image and the chosen text feature.\n",
        "        score = (100.0 * (image_features * text_feature).sum(dim=-1)).sum()\n",
        "        # Backward: gradients wrt the last conv features.\n",
        "        self.model.zero_grad(set_to_none=True)\n",
        "        score.backward(retain_graph=True)\n",
        "        grads = self.gradients  # (N, C, H, W)\n",
        "        acts = self.activations # (N, C, H, W)\n",
        "        # Channel weights by global average pooling over H,W.\n",
        "        weights = grads.mean(dim=(2,3), keepdim=True)\n",
        "        # Weighted sum of activations across channels → class activation map (CAM).\n",
        "        cam = (weights * acts).sum(dim=1, keepdim=False)\n",
        "        cam = torch.relu(cam)\n",
        "        # Normalize CAM to [0, 1] per image.\n",
        "        cams = []\n",
        "        for i in range(cam.shape[0]):\n",
        "            c = cam[i]\n",
        "            c = c - c.min()\n",
        "            denom = c.max().clamp(min=1e-6)\n",
        "            c = c / denom\n",
        "            cams.append(c)\n",
        "        cam = torch.stack(cams, dim=0)\n",
        "        return cam.detach().cpu()\n",
        "\n",
        "\n",
        "def overlay_heatmap(img: Image.Image, cam: np.ndarray, alpha: float = 0.35) -> Image.Image:\n",
        "    # Resize CAM to image size, colorize, and blend with the image for visualization.\n",
        "    img_np = np.array(img.convert(\"RGB\"))\n",
        "    h, w, _ = img_np.shape\n",
        "    cam_resized = cv2.resize(cam, (w, h), interpolation=cv2.INTER_CUBIC)\n",
        "    heat = (cam_resized * 255.0).astype(np.uint8)\n",
        "    heat = cv2.applyColorMap(heat, cv2.COLORMAP_JET)[:, :, ::-1]  # BGR->RGB\n",
        "    overlay = (alpha * heat + (1 - alpha) * img_np).astype(np.uint8)\n",
        "    return Image.fromarray(overlay)\n",
        "\n",
        "# Choose the best prompt for the top label to condition Grad-CAM.\n",
        "\n",
        "def best_prompt_for_label(scores: dict, label: str, label_to_prompts: dict) -> str:\n",
        "    prompts = label_to_prompts[label]\n",
        "    best_p, best_s = None, -1\n",
        "    for p in prompts:\n",
        "        s = scores.get(p, 0.0)\n",
        "        if s > best_s:\n",
        "            best_p, best_s = p, s\n",
        "    return best_p\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_single_text(prompt: str) -> torch.Tensor:\n",
        "    tok = tokenizer([prompt]).to(device)\n",
        "    feat = model.encode_text(tok)\n",
        "    feat = feat / feat.norm(dim=-1, keepdim=True)\n",
        "    return feat\n",
        "\n",
        "# Run Grad-CAM per image using the top predicted label and its best prompt.\n",
        "cam_engine = GradCAM(model)\n",
        "\n",
        "for img_path in image_paths:\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    print(\"\\nGrad-CAM:\", os.path.basename(img_path))\n",
        "    scores = score_image_against_prompts(img, text_features, all_texts)\n",
        "    label_scores = aggregate_by_label(scores, label_to_prompts, reduce=\"mean\")\n",
        "    top_label = max(label_scores.items(), key=lambda x: x[1])[0]\n",
        "    bp = best_prompt_for_label(scores, top_label, label_to_prompts)\n",
        "    print(\"Top label:\", top_label, \"via prompt:\", bp)\n",
        "\n",
        "    tf = encode_single_text(bp)\n",
        "    img_tensor = preprocess_pil(img).to(device)\n",
        "    cam = cam_engine(img_tensor, tf)[0].numpy()\n",
        "    over = overlay_heatmap(img, cam, alpha=0.4)\n",
        "\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,3,1); plt.imshow(img); plt.axis('off'); plt.title('Image')\n",
        "    plt.subplot(1,3,2); plt.imshow(cam, cmap='magma'); plt.axis('off'); plt.title('Grad-CAM')\n",
        "    plt.subplot(1,3,3); plt.imshow(over); plt.axis('off'); plt.title('Overlay')\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "cam_engine.remove()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Ulzmal8doR"
      },
      "source": [
        "### Optional: Prompt calibration with a few labeled images\n",
        "If you have 3–5 labeled images, you can estimate a temperature or per-label bias to calibrate scores.\n",
        "Below is a simple temperature scaling example stub (disabled by default).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1EG8zcn8doR"
      },
      "outputs": [],
      "source": [
        "# Temperature scaling (optional small calibration)\n",
        "# Purpose: learn a single scalar T to rescale logits, improving probability calibration on a tiny labeled set.\n",
        "ENABLE_TEMP = False\n",
        "\n",
        "# Provide a few labeled examples if available: [(image_path, \"true_label\"), ...]\n",
        "labeled_examples = []  # Fill if available\n",
        "\n",
        "def fit_temperature(text_features: torch.Tensor, all_texts: list[str], labeled_examples: list) -> float:\n",
        "    # Optimize a scalar temperature T to minimize negative log-likelihood of the true labels.\n",
        "    T = torch.tensor(1.0, device=device, requires_grad=True)\n",
        "    optimizer = torch.optim.LBFGS([T], lr=0.1, max_iter=50)\n",
        "\n",
        "    def closure():\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_total = 0.0\n",
        "        for img_path, true_label in labeled_examples:\n",
        "            # Recompute probabilities for this image and divide logits by T.\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            image_input = preprocess_pil(img).to(device)\n",
        "            image_features = model.encode_image(image_input)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "            logits = (100.0 * image_features @ text_features.T)[0] / T\n",
        "            probs = logits.softmax(dim=-1)\n",
        "            # Aggregate prompt probabilities per label by summing the prompts.\n",
        "            label_probs = {}\n",
        "            offset = 0\n",
        "            for label, prompts in label_to_prompts.items():\n",
        "                idxs = list(range(offset, offset + len(prompts)))\n",
        "                p = probs[idxs].sum()\n",
        "                label_probs[label] = p\n",
        "                offset += len(prompts)\n",
        "            # Negative log likelihood for the true label.\n",
        "            denom = torch.stack(list(label_probs.values())).sum()\n",
        "            loss = -torch.log(label_probs[true_label] / denom)\n",
        "            loss_total = loss_total + loss\n",
        "        loss_total.backward()\n",
        "        return loss_total\n",
        "\n",
        "    if len(labeled_examples) > 0:\n",
        "        optimizer.step(closure)\n",
        "        return float(T.detach().cpu())\n",
        "    return 1.0\n",
        "\n",
        "if ENABLE_TEMP and labeled_examples:\n",
        "    T_star = fit_temperature(text_features, all_texts, labeled_examples)\n",
        "    print(\"Fitted temperature:\", T_star)\n",
        "else:\n",
        "    T_star = 1.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La9OrbTl8doS"
      },
      "source": [
        "### Commentary and Attribution\n",
        "- This example adapts the CLIP zero-shot classification approach from Radford et al., 2021.\n",
        "- Saliency uses a Grad-CAM adaptation over the RN50 image encoder.\n",
        "- OpenCLIP library used for model weights and preprocessing.\n",
        "- Changes from reference demos: domain-specific prompt templates, per-label prompt aggregation, Grad-CAM overlay, optional temperature scaling.\n",
        "\n",
        "Discuss observed behavior: where prompts succeed, where they fail (domain shift), and how prompt ensembling or calibration helps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iN003Z78doS"
      },
      "source": [
        "Why is this important?\n",
        "- Zero-shot recognition: Shows how text prompts can act as labels, enabling classification without task-specific training.\n",
        "- Label efficiency: Useful when you have few/no annotations; you can still get a usable classifier by crafting prompts.\n",
        "- Interpretability: Grad‑CAM visualizes where the model focuses to support its prediction, aiding trust and error analysis.\n",
        "Prompt engineering: Demonstrates how phrasing/ensembling affects performance and how light calibration can stabilize results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRwQ0ckE8doS"
      },
      "source": [
        "### Educational: Multi‑label scenes and thresholding\n",
        "- Some images contain multiple structures. We can treat zero‑shot as multi‑label by keeping all labels above a threshold.\n",
        "- Below, we show per‑label Grad‑CAM overlays for all labels with probability > 0.15.\n",
        "- Tip: adjust the threshold to trade off precision vs recall.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC_PT4bU8doT"
      },
      "outputs": [],
      "source": [
        "# Multi-label Grad-CAM overlays for labels above a probability threshold\n",
        "threshold = 0.15\n",
        "\n",
        "if len(image_paths) > 0:\n",
        "    sel_idx = 0  # choose an image index\n",
        "    img_path = image_paths[sel_idx]\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    scores = score_image_against_prompts(img, text_features, all_texts)\n",
        "    label_scores = aggregate_by_label(scores, label_to_prompts, reduce=\"mean\")\n",
        "\n",
        "    # Keep labels with prob > threshold\n",
        "    kept = [(lbl, p) for lbl, p in label_scores.items() if p > threshold]\n",
        "    kept.sort(key=lambda x: -x[1])\n",
        "\n",
        "    if len(kept) == 0:\n",
        "        print(\"No labels above threshold. Lower it or choose another image.\")\n",
        "    else:\n",
        "        img_tensor = preprocess_pil(img).to(device)\n",
        "        plt.figure(figsize=(5*len(kept), 4))\n",
        "        for j, (lbl, p) in enumerate(kept):\n",
        "            # best prompt for this label\n",
        "            bp = max(label_to_prompts[lbl], key=lambda prm: scores.get(prm, 0.0))\n",
        "            tf = encode_single_text(bp)\n",
        "            cam = GradCAM(model)(img_tensor, tf)[0].numpy()\n",
        "            over = overlay_heatmap(img, cam, alpha=0.4)\n",
        "\n",
        "            ax = plt.subplot(1, len(kept), j+1)\n",
        "            ax.imshow(over); ax.axis('off')\n",
        "            ax.set_title(f\"{lbl}\\n{p:.2f}\")\n",
        "        plt.tight_layout(); plt.show()\n",
        "else:\n",
        "    print(\"No images found. Add images and rerun.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giTHBw-h8doT"
      },
      "source": [
        "### Educational: Saliency maps (Grad-CAM) — why and how\n",
        "- Why: to localize the evidence for a predicted label. The heatmap shows which image regions most influenced the image–text similarity for the chosen prompt.\n",
        "- How: we backpropagate the image–text score through the RN50 image encoder, weight the last conv feature maps by the pooled gradients (Grad-CAM), and overlay the heatmap on the image.\n",
        "- What to look for: concentrated heat around the anatomical structure (e.g., dura, optic disc). Diffuse or off-target heat can indicate prompt mismatch or domain shift.\n",
        "- Gallery: below we show a 3×N grid per image (row 1: image, row 2: heatmap, row 3: overlay) across N different prompts for the same label to illustrate prompt sensitivity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQTzvGdF8doU"
      },
      "outputs": [],
      "source": [
        "# Per-prompt saliency gallery (3×N) for the top label of a selected image\n",
        "# Choose one image to inspect\n",
        "if len(image_paths) > 0:\n",
        "    sel_idx = 0  # change to view another image\n",
        "    img_path = image_paths[sel_idx]\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    print(\"Image:\", os.path.basename(img_path))\n",
        "\n",
        "    # Score prompts and choose the top label\n",
        "    scores = score_image_against_prompts(img, text_features, all_texts)\n",
        "    label_scores = aggregate_by_label(scores, label_to_prompts, reduce=\"mean\")\n",
        "    top_label, top_prob = max(label_scores.items(), key=lambda x: x[1])\n",
        "    print(\"Top label:\", top_label, f\"({top_prob:.3f})\")\n",
        "\n",
        "    # Pick N prompts for that label (best-first)\n",
        "    prompts_for_label = label_to_prompts[top_label]\n",
        "    prompts_sorted = sorted(prompts_for_label, key=lambda p: scores.get(p, 0.0), reverse=True)\n",
        "    N = min(5, len(prompts_sorted))  # show up to 5 prompts\n",
        "    chosen_prompts = prompts_sorted[:N]\n",
        "\n",
        "    # Build saliency for each chosen prompt\n",
        "    img_tensor = preprocess_pil(img).to(device)\n",
        "    cams = []\n",
        "    overlays = []\n",
        "    for p in chosen_prompts:\n",
        "        tf = encode_single_text(p)\n",
        "        cam = GradCAM(model)(img_tensor, tf)[0].numpy()\n",
        "        over = overlay_heatmap(img, cam, alpha=0.4)\n",
        "        cams.append(cam)\n",
        "        overlays.append(over)\n",
        "\n",
        "    # Plot 3×N: original image row, heatmaps row, overlays row\n",
        "    plt.figure(figsize=(3*N, 9))\n",
        "    for j, p in enumerate(chosen_prompts):\n",
        "        # row 1: image\n",
        "        ax = plt.subplot(3, N, 1 + j)\n",
        "        ax.imshow(img); ax.axis('off')\n",
        "        ax.set_title(f\"{top_label}\")\n",
        "        # row 2: heatmap\n",
        "        ax = plt.subplot(3, N, 1*N + 1 + j)\n",
        "        ax.imshow(cams[j], cmap='magma'); ax.axis('off')\n",
        "        ax.set_title(f\"{j+1}: heat\")\n",
        "        # row 3: overlay\n",
        "        ax = plt.subplot(3, N, 2*N + 1 + j)\n",
        "        ax.imshow(overlays[j]); ax.axis('off')\n",
        "        ax.set_title(f\"{j+1}: {scores.get(chosen_prompts[j], 0.0):.2f}\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "else:\n",
        "    print(\"No images found. Add images to the folder and rerun.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}